{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nblip2_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\\nblip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\").to(device)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers pillow pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "#from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def ensure_direct_image_url(url):\n",
    "    \"\"\"\n",
    "    For imgur: convert non-direct to i.imgur.com/ID.jpg.\n",
    "    All other URLs: return as-is.\n",
    "    \"\"\"\n",
    "    if \"imgur.com\" in url and not re.search(r'\\.(jpg|jpeg|png|gif|bmp|webp|tiff)$', url, re.IGNORECASE):\n",
    "        match = re.search(r'imgur\\.com/(?:gallery/|a/)?([^/?#]+)', url)\n",
    "        if match:\n",
    "            img_id = match.group(1)\n",
    "            return f\"https://i.imgur.com/{img_id}.jpg\"\n",
    "        match = re.search(r'imgur\\.com/([^/?#]+)', url)\n",
    "        if match:\n",
    "            img_id = match.group(1)\n",
    "            return f\"https://i.imgur.com/{img_id}.jpg\"\n",
    "    return url\n",
    "\n",
    "#your logic\n",
    "def smart_download_image(url, save_path):\n",
    "    if \"dropbox.com\" in url:\n",
    "        url = url.replace(\"?dl=0\", \"\")\n",
    "        if \"?raw=1\" not in url:\n",
    "            if \"?\" in url:\n",
    "                url += \"&raw=1\"\n",
    "            else:\n",
    "                url += \"?raw=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\",\n",
    "        \"Referer\": url,\n",
    "        \"Accept-Encoding\": \"identity\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        if resp.status_code == 200 and resp.headers.get('content-type', '').startswith(\"image\"):\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed (status {resp.status_code}, type {resp.headers.get('content-type', '')}) for {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download error for {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "\"\"\"\n",
    "blip2_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\").to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from typing import List\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_edit_instructions(\n",
    "    base_caption: str,\n",
    "    num_instructions: int,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    temperature: float = 0.7\n",
    ") -> List[str]:\n",
    "    prompt = (\n",
    "        f\"You are an image editing assistant. Given the image caption: '{base_caption}', \"\n",
    "        f\"generate {num_instructions} concise and diverse edit instructions \"\n",
    "        f\"that could be applied to the image. Return the instructions as a JSON array of strings.\"\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You generate creative image edit instructions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=num_instructions * 25,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    instructions = json.loads(content)\n",
    "    \n",
    "    if not isinstance(instructions, list):\n",
    "        raise ValueError(\"Parsed JSON is not list\")\n",
    "    return instructions\n",
    "\n",
    "# def generate_edit_instructions(\n",
    "#     base_caption: str,\n",
    "#     num_instructions: int,\n",
    "#     model: str = \"gpt-3.5-turbo\",\n",
    "#     temperature: float = 0.7\n",
    "# ) -> List[str]:\n",
    "#     instructions: List[str] = []\n",
    "#     system_msg = {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You generate creative image edit instructions.\"\n",
    "#     }\n",
    "\n",
    "#     for i in range(num_instructions):\n",
    "#         user_prompt = (\n",
    "#             f\"You are an image editing assistant. Given the image caption: '{base_caption}', \"\n",
    "#             \"generate an edit instruction that could be applied to the image. \"\n",
    "#         )\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=model,\n",
    "#             messages=[system_msg, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "#             temperature=temperature,\n",
    "#             max_tokens=50,\n",
    "#         )\n",
    "#         content = response.choices[0].message.content.strip()\n",
    "#         instructions.append(instr)\n",
    "\n",
    "#     return instructions\n",
    "    \n",
    "\n",
    "def generate_diff_instruction(\n",
    "    base_caption: str,\n",
    "    edited_caption: str,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    prompt = (\n",
    "        f\"Original caption: '{base_caption}'\\n\"\n",
    "        f\"Edited caption: '{edited_caption}'\\n\"\n",
    "        f\"Write a concise instruction that describes the visual changes applied to the image.\"\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You summarize image edits as text instructions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=60,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:00<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_blip_caption(image_path, blip_processor, blip_model):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device)\n",
    "        output = blip_model.generate(**inputs, max_length=50, num_beams=11,length_penalty=1.7, repetition_penalty=1.4,early_stopping=True, do_sample=False)\n",
    "        caption = blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"BLIP failed for {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\"\"\"\n",
    "def generate_blip2_flan_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = \"Describe this image in extreme detail:\"\n",
    "    inputs = blip2_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    output = blip2_model.generate(**inputs, max_new_tokens=35)\n",
    "    caption = blip2_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"RealEdit_train_split_urls.csv\")\n",
    "\n",
    "output = []\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "N = 10\n",
    "for i, row in tqdm(df.iterrows(), total=min(len(df), N)):\n",
    "    if i >= N: break\n",
    "    orig_url = str(row[\"input_url\"])\n",
    "    img_url = ensure_direct_image_url(orig_url)\n",
    "    img_name = row[\"input_image_name\"]\n",
    "    edit_request = row['instruction']\n",
    "    local_path = f\"images/{img_name}\"\n",
    "    caption = \"\"\n",
    "    if smart_download_image(img_url, local_path):\n",
    "        caption = generate_blip_caption(local_path, blip_processor, blip_model)\n",
    "        #caption = generate_blip2_flan_caption(local_path)\n",
    "    if 'is no longer available' in caption:\n",
    "        os.remove(local_path)\n",
    "        continue\n",
    "    output.append({\n",
    "        \"input_image_name\": img_name,\n",
    "        \"input_url\": orig_url,\n",
    "        \"download_url\": img_url,\n",
    "        \"download_success\": os.path.exists(local_path) and os.path.getsize(local_path) > 0,\n",
    "        \"caption\": caption,\n",
    "        \"edit_request\": edit_request\n",
    "    })\n",
    "\n",
    "ff = pd.DataFrame(output)\n",
    "\n",
    "ff.to_csv(\"captions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from api_key import OPENAI_API_KEY\n",
    "def generate_edit_instructions(\n",
    "    base_caption: str,\n",
    "    example_request: str,\n",
    "    num_instructions: int,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    temperature: float = 0.7\n",
    ") -> List[str]:\n",
    "    prompt = (\n",
    "        f\"You are an image editing assistant. Given the image caption: '{base_caption}', \"\n",
    "        f\"generate {num_instructions} concise and diverse edit instructions \"\n",
    "        f\"that could be applied to the image. \"\n",
    "        f\"Use this example edit instruction as a reference: '{example_request}'. \"\n",
    "        f\"Return the instructions as a JSON array of strings.\"\n",
    "    )\n",
    "    client = openai.OpenAI(api_key = OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You generate creative image edit instructions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=num_instructions * 25,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    instructions = json.loads(content)\n",
    "    \n",
    "    if not isinstance(instructions, list):\n",
    "        raise ValueError(\"Parsed JSON is not list\")\n",
    "    return instructions\n",
    "\n",
    "'''\n",
    "img_to_edit = {}\n",
    "for i, row in ff.iterrows():\n",
    "    caption = row['caption']\n",
    "    example_request = row['edit_request']\n",
    "    img_name = row['input_image_name']\n",
    "    img_to_edit[img_name] = list(generate_edit_instructions(caption, example_request, 5))\n",
    "\n",
    "print(img_to_edit)\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
