{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "vFdXPQ06cezJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0fCmC_AcbK_"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256,expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "JCWaDsZtcbsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests, json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "import openai\n",
        "openai.api_key = \"openai_APIKEY\"\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "for module in pipe.text_encoder.modules():\n",
        "    if hasattr(module, \"inplace\") and module.inplace:\n",
        "        module.inplace = False\n",
        "\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_attention_slicing()\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "XzTrx5Kdcc_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def ensure_direct_image_url(url):\n",
        "    \"\"\"Convert Imgur/Dropbox URLs to direct image links if needed.\"\"\"\n",
        "    if \"imgur.com\" in url and not re.search(r'\\.(jpg|jpeg|png|bmp)$', url):\n",
        "        m = re.search(r'imgur\\.com/(?:gallery/|a/)?([^.?&]+)', url)\n",
        "        if m: return f\"https://i.imgur.com/{m.group(1)}.jpg\"\n",
        "    return url\n",
        "\n",
        "def smart_download_image(url, save_path):\n",
        "    \"\"\"Download an image with user-agent header, handling Dropbox links.\"\"\"\n",
        "    if \"dropbox.com\" in url:\n",
        "        url = url.replace(\"?dl=0\", \"\")\n",
        "        if \"?raw=1\" not in url:\n",
        "            url += \"&raw=1\" if \"?\" not in url else \"&raw=1\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept-Encoding\": \"identity\"}\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=30)\n",
        "        if resp.status_code == 200 and resp.headers.get('content-type','').startswith(\"image\"):\n",
        "            with open(save_path, \"wb\") as f: f.write(resp.content)\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"Download error for {url}: {e}\")\n",
        "    return False\n",
        "\n",
        "df = pd.read_csv(\"RealEdit_train_split_urls.csv\")\n",
        "os.makedirs(\"originals\", exist_ok=True)\n",
        "\n",
        "N = 10\n",
        "image_info = []\n",
        "for i, row in tqdm(df.iterrows(), total=min(len(df), N)):\n",
        "    if i >= N: break\n",
        "    fname = row[\"input_image_name\"]\n",
        "    url = ensure_direct_image_url(str(row[\"input_url\"]))\n",
        "    save_path = f\"originals/{fname}\"\n",
        "    if smart_download_image(url, save_path):\n",
        "        image_info.append((fname, url, row.get(\"subreddit\",\"\"), str(row.get(\"title\",\"\")), str(row.get(\"selftext\",\"\"))))\n",
        "    else:\n",
        "        print(f\"Skipping {fname}: download failed.\")\n"
      ],
      "metadata": {
        "id": "Z2zMgK80cl99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTokenizer"
      ],
      "metadata": {
        "id": "dGCCqg8Zcsix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import base64"
      ],
      "metadata": {
        "id": "fXAtsJbNcuQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "def truncate_caption_safely(caption, max_tokens=77):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', caption.strip())\n",
        "\n",
        "    current_text = \"\"\n",
        "    for sentence in sentences:\n",
        "        proposed_text = (current_text + \" \" + sentence).strip()\n",
        "        token_ids = tokenizer(proposed_text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
        "        if len(token_ids) > max_tokens:\n",
        "            break\n",
        "        current_text = proposed_text\n",
        "\n",
        "    return current_text\n",
        "\n",
        "\n",
        "captions = []\n",
        "client = OpenAI(api_key=openai.api_key)\n",
        "\n",
        "image_dir = \"originals\"\n",
        "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))][:N]\n",
        "captions = []\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    if not os.path.exists(image_path) or os.path.getsize(image_path) < 1024:\n",
        "        print(f\"Skipping {image_file}: file missing or too small.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            image_bytes = img_file.read()\n",
        "            base64_img = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Describe the image in full detail, but limit your response to under 50 words. Focus on what's visually clear. Avoid exaggeration or hallucination. Do not include information that is not clearly visible in the image.\"},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=77\n",
        "        )\n",
        "\n",
        "        caption = response.choices[0].message.content.strip()\n",
        "        caption = truncate_caption_safely(caption)\n",
        "        captions.append((image_file, caption))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{image_file}, Error: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "fFJJ-Ns_cwi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "df = pd.DataFrame(captions, columns=[\"Image\", \"Caption\"])\n",
        "print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=True))"
      ],
      "metadata": {
        "id": "SSklN0Rtcyy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "bzW1JrzGc0wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "3E5D_NUqc2TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_edits = {}\n",
        "for (fname, cap) in captions:\n",
        "    print(f\"\\nGenerating edits for '{fname}' with caption: {cap}\")\n",
        "    prompt = (\n",
        "    \"You are simulating real user editing behavior for a dataset of image edits.\\n\"\n",
        "    \"Given a description of an image, imagine how actual users would ask to modify it. \"\n",
        "    \"These edits should be creative, realistic, and specific — things a person might type into an AI editor, like:\\n\"\n",
        "    \"- 'Add a dog sitting near the woman'\\n\"\n",
        "    \"- 'Make the sunset more vibrant'\\n\"\n",
        "    \"- 'Change the man’s outfit to a business suit'\\n\"\n",
        "    \"- 'Remove the second person from the left'\\n\"\n",
        "    \"- 'Make the child look older'\\n\"\n",
        "    \"Each edit should involve a meaningful visual change to the image, not just generic filters like 'increase contrast'.\\n\"\n",
        "    \"\\n\"\n",
        "    \"For each instruction, generate a matching edited image caption that describes the image *after* the edit.\\n\"\n",
        "    \"Each 'edited_caption' must be **under 77 tokens**, even after tokenization (not just word count).\\n\"\n",
        "    \"Avoid repetitions. The 10 edits must be diverse (e.g. subject, background, object-level, style).\\n\"\n",
        "    \"\\n\"\n",
        "    \"Output a JSON array of 10 items, where each item is an object with two fields:\\n\"\n",
        "    \"- 'instruction': the user's edit request\\n\"\n",
        "    \"- 'edited_caption': the caption for the image after applying that edit\\n\"\n",
        "    \"Do not include any explanation. Return only the JSON array.\\n\\n\"\n",
        "    f\"Image Description: \\\"{cap}\\\"\"\n",
        "    )\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.8\n",
        "        )\n",
        "        gpt_output = response.choices[0].message.content\n",
        "        start = gpt_output.find('[')\n",
        "        end = gpt_output.rfind(']') + 1\n",
        "        edits = json.loads(gpt_output[start:end])\n",
        "    except Exception as e:\n",
        "        print(f\"GPT API call failed for {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if not isinstance(edits, list) or len(edits) != 10:\n",
        "        print(f\"Unexpected format for {fname}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    filtered_edits = []\n",
        "    for item in edits:\n",
        "        tok_len = len(enc.encode(item[\"edited_caption\"]))\n",
        "        if tok_len < 77:\n",
        "            filtered_edits.append(item)\n",
        "        else:\n",
        "            print(f\"Skipping overlong caption (len={tok_len}) for '{fname}': {item['edited_caption']}\")\n",
        "\n",
        "    if len(filtered_edits) < 10:\n",
        "        print(f\"Only {len(filtered_edits)} valid edits (under 77 tokens) for {fname}.\")\n",
        "\n",
        "    all_edits[fname] = filtered_edits\n",
        "\n",
        "    for idx, item in enumerate(filtered_edits, 1):\n",
        "        print(f\" {idx}. INSTR: {item['instruction']} | CAPTION: {item['edited_caption']}\")"
      ],
      "metadata": {
        "id": "dt-LEZZqc363"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}