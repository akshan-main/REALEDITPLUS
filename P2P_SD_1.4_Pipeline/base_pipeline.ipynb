{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256,expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\\nfor module in pipe.text_encoder.modules():\\n    if hasattr(module, \"inplace\") and module.inplace:\\n        module.inplace = False\\n\\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\\npipe.enable_attention_slicing()\\ntry:\\n    pipe.enable_xformers_memory_efficient_attention()\\nexcept Exception:\\n    pass\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, requests, json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "openai.api_key = 'insert key here'\n",
    "\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel\n",
    "'''\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "for module in pipe.text_encoder.modules():\n",
    "    if hasattr(module, \"inplace\") and module.inplace:\n",
    "        module.inplace = False\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_attention_slicing()\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "except Exception:\n",
    "    pass\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:02,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 3v1cvp.jpg: download failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def ensure_direct_image_url(url):\n",
    "    \"\"\"Convert Imgur/Dropbox URLs to direct image links if needed.\"\"\"\n",
    "    if \"imgur.com\" in url and not re.search(r'\\.(jpg|jpeg|png|bmp)$', url):\n",
    "        m = re.search(r'imgur\\.com/(?:gallery/|a/)?([^.?&]+)', url)\n",
    "        if m: return f\"https://i.imgur.com/{m.group(1)}.jpg\"\n",
    "    return url\n",
    "\n",
    "def smart_download_image(url, save_path):\n",
    "    \"\"\"Download an image with user-agent header, handling Dropbox links.\"\"\"\n",
    "    if \"dropbox.com\" in url:\n",
    "        url = url.replace(\"?dl=0\", \"\")\n",
    "        if \"?raw=1\" not in url:\n",
    "            url += \"&raw=1\" if \"?\" not in url else \"&raw=1\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept-Encoding\": \"identity\"}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        if resp.status_code == 200 and resp.headers.get('content-type','').startswith(\"image\"):\n",
    "            with open(save_path, \"wb\") as f: f.write(resp.content)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Download error for {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "df = pd.read_csv(\"RealEdit_train_split_urls.csv\")\n",
    "os.makedirs(\"originals\", exist_ok=True)\n",
    "\n",
    "N = 10\n",
    "image_info = []\n",
    "for i, row in tqdm(df.iterrows(), total=min(len(df), N)):\n",
    "    if i >= N: break\n",
    "    fname = row[\"input_image_name\"]\n",
    "    url = ensure_direct_image_url(str(row[\"input_url\"]))\n",
    "    save_path = f\"originals/{fname}\"\n",
    "    if smart_download_image(url, save_path):\n",
    "        image_info.append((fname, url, row.get(\"subreddit\",\"\"), str(row.get(\"title\",\"\")), str(row.get(\"selftext\",\"\"))))\n",
    "    else:\n",
    "        print(f\"Skipping {fname}: download failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 3vn0dc.jpeg: file missing or too small.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "def truncate_caption_safely(caption, max_tokens=77):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', caption.strip())\n",
    "\n",
    "    current_text = \"\"\n",
    "    for sentence in sentences:\n",
    "        proposed_text = (current_text + \" \" + sentence).strip()\n",
    "        token_ids = tokenizer(proposed_text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "        if len(token_ids) > max_tokens:\n",
    "            break\n",
    "        current_text = proposed_text\n",
    "\n",
    "    return current_text\n",
    "\n",
    "\n",
    "captions = []\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "image_dir = \"originals\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))][:N]\n",
    "captions = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    if not os.path.exists(image_path) or os.path.getsize(image_path) < 1024:\n",
    "        print(f\"Skipping {image_file}: file missing or too small.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            image_bytes = img_file.read()\n",
    "            base64_img = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe the image in full detail, but limit your response to under 50 words. Focus on what's visually clear. Avoid exaggeration or hallucination. Do not include information that is not clearly visible in the image.\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=77\n",
    "        )\n",
    "\n",
    "        caption = response.choices[0].message.content.strip()\n",
    "        caption = truncate_caption_safely(caption)\n",
    "        captions.append((image_file, caption))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{image_file}, Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Image       | Caption                                                                                                                                                                                                                                                                   |\n",
      "|----|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | 3vtbmy.jpg  | A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless, wearing shorts. The setting sun casts a muted light over the scene.                                            |\n",
      "|  1 | 3vo9iy.jpg  | A monochrome image shows a group of ten people, consisting of seven women and three men. They're dressed in formal and semi-formal attire, posing outdoors. The background has foliage and a pergola structure.                                                           |\n",
      "|  2 | 3vg97p.jpg  | A person in a military uniform walks between two rows of sandbags stacked in high walls. The area has several barrels and structures with metal roofs, creating a corridor-like path. The image is faded with bright lighting.                                            |\n",
      "|  3 | 3vupca.jpeg | A sepia-toned portrait of seven individuals, four men and a woman in the back row, two seated adults in the front. The men wear suits, some with bow ties. The woman in the back wears a checked dress with a collar, and the woman in front has a beaded trim.           |\n",
      "|  4 | 3v2ru0.jpeg | A faded, worn black-and-white photo shows three people: two adults and a child. The adults are seated, smiling, with the woman wearing a floral dress. The child is partially visible in the foreground. The image is cracked and textured with visible damage.           |\n",
      "|  5 | 3vqxg7.jpeg | A black-and-white photo shows a man holding a smiling young girl aloft. The girl is wearing a striped shirt, pants, and sneakers. The man is dressed in a button-up shirt, smiling at the child. The background is an outdoor setting with blurred foliage and mountains. |\n",
      "|  6 | 3vo49o.jpg  | A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.                                                   |\n",
      "|  7 | 3vtoea.jpeg | A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.                                            |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "df = pd.DataFrame(captions, columns=[\"Image\", \"Caption\"])\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating edits for '3vtbmy.jpg' with caption: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless, wearing shorts. The setting sun casts a muted light over the scene.\n",
      " 1. INSTR: Add a dog running towards the couple | CAPTION: A couple stands on a beach, with waves in the background. A dog runs towards them under the setting sun.\n",
      " 2. INSTR: Change the person on the left's outfit to a formal dress | CAPTION: A couple stands on a beach, with waves in the background. The person on the left wears a formal dress, while the person on the right is shirtless.\n",
      " 3. INSTR: Remove the waves in the background | CAPTION: A couple stands on a beach, with no waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless.\n",
      " 4. INSTR: Make the setting sun bright yellow | CAPTION: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless. The bright yellow sun shines over the scene.\n",
      " 5. INSTR: Add a beach umbrella behind the couple | CAPTION: A couple stands on a beach, with waves in the background. A beach umbrella stands behind them under the setting sun.\n",
      " 6. INSTR: Change the person on the right's shorts to swimming trunks | CAPTION: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right wears swimming trunks.\n",
      " 7. INSTR: Remove the person on the left | CAPTION: A person stands on a beach, with waves in the background. The person on the right is shirtless, wearing shorts. The setting sun casts a muted light over the scene.\n",
      " 8. INSTR: Make the sky cloudy instead of clear | CAPTION: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless. The cloudy sky adds drama to the scene.\n",
      " 9. INSTR: Add seagulls flying above the couple | CAPTION: A couple stands on a beach, with waves in the background. Seagulls fly above them under the setting sun.\n",
      " 10. INSTR: Change the lighting to a sunset glow | CAPTION: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless. The scene is bathed in a warm sunset glow.\n",
      "\n",
      "Generating edits for '3vo9iy.jpg' with caption: A monochrome image shows a group of ten people, consisting of seven women and three men. They're dressed in formal and semi-formal attire, posing outdoors. The background has foliage and a pergola structure.\n",
      " 1. INSTR: Change the color of all clothing to bright neon colors | CAPTION: A vibrant image displays a group of ten people, with neon-colored clothing, posing outdoors near foliage and a pergola.\n",
      " 2. INSTR: Add a flock of colorful birds flying in the background | CAPTION: A monochrome image shows a group of people posing outdoors with a flock of colorful birds flying in the background.\n",
      " 3. INSTR: Remove all the men from the image | CAPTION: A monochrome image showcases a group of seven women posing outdoors, surrounded by foliage and a pergola structure.\n",
      " 4. INSTR: Give everyone in the image sunglasses | CAPTION: A stylish image reveals a group of ten people wearing sunglasses, posing outdoors near foliage and a pergola structure.\n",
      " 5. INSTR: Add a rainbow in the sky as a backdrop | CAPTION: A colorful image features a group of ten people posing outdoors with a rainbow in the sky, foliage, and a pergola structure.\n",
      " 6. INSTR: Change the hairstyle of all individuals to long flowing hair | CAPTION: A transformed image displays a group of ten people with long flowing hair, dressed in formal and semi-formal attire, posing outdoors near foliage and a pergola structure.\n",
      " 7. INSTR: Make the foliage in the background appear to be in full bloom | CAPTION: A picturesque image shows a group of ten people posing outdoors with vibrant, blooming foliage and a pergola structure in the background.\n",
      " 8. INSTR: Add a vintage film grain effect to the image | CAPTION: A retro image depicts a group of ten people, dressed in formal and semi-formal attire, posing outdoors near foliage and a pergola structure.\n",
      " 9. INSTR: Change the time of day to dusk with a moon in the sky | CAPTION: A twilight image captures a group of ten people, dressed in formal and semi-formal attire, posing outdoors near foliage and a pergola structure.\n",
      " 10. INSTR: Give each person a different colored umbrella | CAPTION: A whimsical image features a group of ten people, each holding a unique colored umbrella, posing outdoors near foliage and a pergola structure.\n",
      "\n",
      "Generating edits for '3vg97p.jpg' with caption: A person in a military uniform walks between two rows of sandbags stacked in high walls. The area has several barrels and structures with metal roofs, creating a corridor-like path. The image is faded with bright lighting.\n",
      " 1. INSTR: Add a military helicopter flying overhead | CAPTION: A person in a military uniform walks between rows of sandbags with a military helicopter flying overhead.\n",
      " 2. INSTR: Make the sandbags covered in green moss | CAPTION: A person in a military uniform walks between rows of moss-covered sandbags stacked in high walls.\n",
      " 3. INSTR: Change the metal roofs to wooden shacks | CAPTION: A person in a military uniform walks between rows of sandbags with wooden shacks instead of metal roofs.\n",
      " 4. INSTR: Remove the barrels from the scene | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls, without any barrels present.\n",
      " 5. INSTR: Add barbed wire on top of the sandbag walls | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls with barbed wire on top.\n",
      " 6. INSTR: Change the faded effect to a vintage sepia tone | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls. The area has a vintage sepia tone.\n",
      " 7. INSTR: Increase the shadows to create a more dramatic effect | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls with increased shadows for a dramatic effect.\n",
      " 8. INSTR: Add a military dog standing guard | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls with a military dog standing guard.\n",
      " 9. INSTR: Change the bright lighting to a moody overcast sky | CAPTION: A person in a military uniform walks between two rows of sandbags stacked in high walls under a moody overcast sky.\n",
      " 10. INSTR: Make the person's uniform camouflage patterned | CAPTION: A person in a camouflage-patterned military uniform walks between two rows of sandbags stacked in high walls.\n",
      "\n",
      "Generating edits for '3vupca.jpeg' with caption: A sepia-toned portrait of seven individuals, four men and a woman in the back row, two seated adults in the front. The men wear suits, some with bow ties. The woman in the back wears a checked dress with a collar, and the woman in front has a beaded trim.\n",
      " 1. INSTR: Change the man's suit color to navy blue | CAPTION: A sepia-toned portrait of seven individuals, four men wearing navy blue suits and a woman in the back row.\n",
      " 2. INSTR: Add a vintage camera hanging from one man's neck | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row, two seated adults in the front, one man with a vintage camera hanging from his neck.\n",
      " 3. INSTR: Replace the woman's dress pattern with floral print | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row wearing floral print dresses.\n",
      " 4. INSTR: Make the background blurry to focus on the front row | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in focus in the front row, with a blurred background.\n",
      " 5. INSTR: Change the lighting to create a dramatic shadow effect | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman with dramatic shadow effects.\n",
      " 6. INSTR: Remove the bow tie from one of the men | CAPTION: A sepia-toned portrait of seven individuals, four men without bow ties and a woman in the back row.\n",
      " 7. INSTR: Add a reflection on the floor as if they are standing on water | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row, with a reflection on the floor as if standing on water.\n",
      " 8. INSTR: Change the woman's hairstyle to a sleek updo | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row with a sleek updo hairstyle.\n",
      " 9. INSTR: Replace the beaded trim on the woman in front with lace | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row, and a woman in front with a lace trim.\n",
      " 10. INSTR: Add a subtle vignette effect to frame the image | CAPTION: A sepia-toned portrait of seven individuals, four men and a woman in the back row, two seated adults in the front, with a subtle vignette effect framing the image.\n",
      "\n",
      "Generating edits for '3v2ru0.jpeg' with caption: A faded, worn black-and-white photo shows three people: two adults and a child. The adults are seated, smiling, with the woman wearing a floral dress. The child is partially visible in the foreground. The image is cracked and textured with visible damage.\n",
      " 1. INSTR: Colorize the photo and enhance the colors of the floral dress | CAPTION: A vibrant photo displays three people: two adults seated, smiling, the woman in a colorful floral dress, and a child in the foreground.\n",
      " 2. INSTR: Add a bouquet of flowers in the woman's hand | CAPTION: A faded photo shows three people: two adults seated, smiling, with the woman holding a bouquet of flowers, the child partially visible.\n",
      " 3. INSTR: Repair the cracks and damage on the photo | CAPTION: A clear photo depicts three people: two adults seated, smiling, with the child partially visible in the foreground.\n",
      " 4. INSTR: Change the woman's dress to a modern outfit | CAPTION: A faded photo features three people: two adults seated, smiling, the woman in a modern outfit, the child partially visible.\n",
      " 5. INSTR: Insert a vintage frame around the photo | CAPTION: A faded photo framed in a vintage frame shows three people: two adults seated, smiling, the child partially visible.\n",
      " 6. INSTR: Turn the background into a sunny beach scene | CAPTION: A faded photo shows three people: two adults seated, smiling, with the woman in a floral dress, the child partially visible on a sunny beach.\n",
      " 7. INSTR: Add a pet cat sitting next to the adults | CAPTION: A faded photo shows three people: two adults seated, smiling, with a pet cat beside them, the child partially visible.\n",
      " 8. INSTR: Remove the texture and make the photo glossy | CAPTION: A glossy photo displays three people: two adults seated, smiling, the woman in a floral dress, the child partially visible.\n",
      " 9. INSTR: Place a rainbow in the background of the photo | CAPTION: A faded photo shows three people: two adults seated, smiling, with the woman in a floral dress, a rainbow in the background.\n",
      " 10. INSTR: Enlarge the child to be the focal point of the image | CAPTION: A faded photo highlights the child as the focal point, with two adults seated, smiling, the woman in a floral dress.\n",
      "\n",
      "Generating edits for '3vqxg7.jpeg' with caption: A black-and-white photo shows a man holding a smiling young girl aloft. The girl is wearing a striped shirt, pants, and sneakers. The man is dressed in a button-up shirt, smiling at the child. The background is an outdoor setting with blurred foliage and mountains.\n",
      " 1. INSTR: Add a colorful hot air balloon in the sky | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft with a colorful hot air balloon in the sky.\n",
      " 2. INSTR: Change the man's shirt to a Hawaiian print | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The man is dressed in a Hawaiian print shirt, smiling at the child.\n",
      " 3. INSTR: Make the mountains snow-capped | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The background is an outdoor setting with snow-capped mountains.\n",
      " 4. INSTR: Transform the striped shirt into a polka dot pattern | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The girl is wearing a polka dot shirt, pants, and sneakers.\n",
      " 5. INSTR: Add a flock of birds flying in the background | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The background is an outdoor setting with a flock of birds flying.\n",
      " 6. INSTR: Give the man sunglasses | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The man is dressed in a button-up shirt, smiling at the child, wearing sunglasses.\n",
      " 7. INSTR: Change the child's expression to a look of surprise | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The girl is wearing a striped shirt, pants, and sneakers, looking surprised.\n",
      " 8. INSTR: Place a picnic basket next to the man | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. There is a picnic basket next to the man, smiling at the child.\n",
      " 9. INSTR: Add a rainbow in the background | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The background is an outdoor setting with a rainbow.\n",
      " 10. INSTR: Change the foliage to cherry blossom trees | CAPTION: A black-and-white photo shows a man holding a smiling young girl aloft. The background is an outdoor setting with cherry blossom trees.\n",
      "\n",
      "Generating edits for '3vo49o.jpg' with caption: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.\n",
      " 1. INSTR: Replace the wooden house with a modern skyscraper | CAPTION: A sepia-toned image shows a family with a dog in front of a modern skyscraper. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.\n",
      " 2. INSTR: Add a rainbow stretching across the sky | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage and a rainbow stretching across the sky.\n",
      " 3. INSTR: Change the dog to a cat lounging on the hammock | CAPTION: A sepia-toned image shows a family with a cat lounging on the hammock in front of a wooden house. Trees surround them. Three adults and two children are visible. The scene is outdoors with lush foliage.\n",
      " 4. INSTR: Make one of the adults wear a cowboy hat | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Two adults and two children are visible, with one adult wearing a cowboy hat. The scene is outdoors with lush foliage.\n",
      " 5. INSTR: Remove one child from the scene | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and one child are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.\n",
      " 6. INSTR: Change the lush foliage to a desert landscape | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with a desert landscape.\n",
      " 7. INSTR: Add a hot air balloon floating in the background | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage and a hot air balloon floating in the background.\n",
      " 8. INSTR: Change the sepia tone to a black and white filter | CAPTION: A black and white image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.\n",
      " 9. INSTR: Make the dog larger than the adults | CAPTION: A sepia-toned image shows a family with a large dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with lush foliage.\n",
      " 10. INSTR: Change the scene to a snowy winter landscape | CAPTION: A sepia-toned image shows a family with a dog in front of a wooden house. Trees surround them. Three adults and two children are visible, with one adult sitting in a hammock. The scene is outdoors with a snowy winter landscape.\n",
      "\n",
      "Generating edits for '3vtoea.jpeg' with caption: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.\n",
      " 1. INSTR: Add a red balloon tied to the picnic bench | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white. A red balloon is tied to the picnic bench.\n",
      " 2. INSTR: Change the wooden picnic bench to a colorful blanket | CAPTION: A child and an adult are kneeling by a colorful blanket. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.\n",
      " 3. INSTR: Remove the soda cans from the table | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. The scene appears to be outdoors, captured in black and white.\n",
      " 4. INSTR: Add a cat sitting on the picnic bench | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white. A cat is sitting on the picnic bench.\n",
      " 5. INSTR: Change the rifle to a fishing rod | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a fishing rod, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.\n",
      " 6. INSTR: Make the adult wear a cowboy hat | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult wearing a cowboy hat. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.\n",
      " 7. INSTR: Change the scene to a snowy landscape | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be in a snowy landscape, captured in black and white.\n",
      " 8. INSTR: Add a bird perched on the child's shoulder | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white. A bird is perched on the child's shoulder.\n",
      " 9. INSTR: Remove the adult from the image | CAPTION: A child is kneeling by a wooden picnic bench. The child is holding a rifle. Soda cans are placed along the table. The scene appears to be outdoors, captured in black and white.\n",
      " 10. INSTR: Make the image appear as a vintage sepia-toned photograph | CAPTION: A child and an adult are kneeling by a wooden picnic bench. The child is holding a rifle, with assistance from the adult. Soda cans are placed along the table. The scene appears to be outdoors, captured in sepia tones.\n"
     ]
    }
   ],
   "source": [
    "all_edits = {}\n",
    "cap_and_edits = {}\n",
    "for (fname, cap) in captions:\n",
    "    print(f\"\\nGenerating edits for '{fname}' with caption: {cap}\")\n",
    "    prompt = (\n",
    "    \"You are simulating real user editing behavior for a dataset of image edits.\\n\"\n",
    "    \"Given a description of an image, imagine how actual users would ask to modify it. \"\n",
    "    \"These edits should be creative, realistic, and specific — things a person might type into an AI editor, like:\\n\"\n",
    "    \"- 'Add a dog sitting near the woman'\\n\"\n",
    "    \"- 'Make the sunset more vibrant'\\n\"\n",
    "    \"- 'Change the man’s outfit to a business suit'\\n\"\n",
    "    \"- 'Remove the second person from the left'\\n\"\n",
    "    \"- 'Make the child look older'\\n\"\n",
    "    \"Each edit should involve a meaningful visual change to the image, not just generic filters like 'increase contrast'.\\n\"\n",
    "    \"\\n\"\n",
    "    \"For each instruction, generate a matching edited image caption that describes the image *after* the edit.\\n\"\n",
    "    \"Each 'edited_caption' must be **under 77 tokens**, even after tokenization (not just word count).\\n\"\n",
    "    \"Avoid repetitions. The 10 edits must be diverse (e.g. subject, background, object-level, style).\\n\"\n",
    "    \"\\n\"\n",
    "    \"Output a JSON array of 10 items, where each item is an object with two fields:\\n\"\n",
    "    \"- 'instruction': the user's edit request\\n\"\n",
    "    \"- 'edited_caption': the caption for the image after applying that edit\\n\"\n",
    "    \"Do not include any explanation. Return only the JSON array.\\n\\n\"\n",
    "    f\"Image Description: \\\"{cap}\\\"\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8\n",
    "        )\n",
    "        gpt_output = response.choices[0].message.content\n",
    "        start = gpt_output.find('[')\n",
    "        end = gpt_output.rfind(']') + 1\n",
    "        edits = json.loads(gpt_output[start:end])\n",
    "    except Exception as e:\n",
    "        print(f\"GPT API call failed for {fname}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if not isinstance(edits, list) or len(edits) != 10:\n",
    "        print(f\"Unexpected format for {fname}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    filtered_edits = []\n",
    "    for item in edits:\n",
    "        tok_len = len(enc.encode(item[\"edited_caption\"]))\n",
    "        if tok_len < 77:\n",
    "            filtered_edits.append(item)\n",
    "        else:\n",
    "            print(f\"Skipping overlong caption (len={tok_len}) for '{fname}': {item['edited_caption']}\")\n",
    "\n",
    "    if len(filtered_edits) < 10:\n",
    "        print(f\"Only {len(filtered_edits)} valid edits (under 77 tokens) for {fname}.\")\n",
    "\n",
    "    all_edits[fname] = filtered_edits\n",
    "\n",
    "    cap_and_edits[fname] = []\n",
    "    cap_and_edits[fname].append(cap)\n",
    "    cap_and_edits[fname].append([])\n",
    "\n",
    "    for idx, item in enumerate(filtered_edits, 1):\n",
    "        print(f\" {idx}. INSTR: {item['instruction']} | CAPTION: {item['edited_caption']}\")\n",
    "        cap_and_edits[fname][1].append((item['instruction'], item['edited_caption']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "#from diffusers import StableDiffusion3Pipeline\n",
    "#from diffusers import DiffusionPipeline\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'use_auth_token': '<replace with your token>'} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4e0ac0d1a84c4cbd4f1d82b8115ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MY_TOKEN = '<replace with your token>'\n",
    "LOW_RESOURCE = False\n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN).to(device)\n",
    "#ldm_stable = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\")\n",
    "#ldm_stable = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16)\n",
    "#ldm_stable = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\", torch_dtype=torch.bfloat16)\n",
    "#ldm_stable = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\")\n",
    "#ldm_stable = ldm_stable.to(device)\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    \n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16 ** 2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(len(values), 77)\n",
    "    values = torch.tensor(values, dtype=torch.float32)\n",
    "    for word in word_select:\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = values\n",
    "    return equalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    #ptp_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/architg/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:90: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (1, model.unet.in_channels, height // 8, width // 8),\n",
      "/Users/architg/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:93: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  latents = latent.expand(batch_size,  model.unet.in_channels, height // 8, width // 8).to(model.device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1cb543a5e4a858a91ac3623be331c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA painting of a squirrel eating a burger\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m controller \u001b[38;5;241m=\u001b[39m AttentionStore()\n\u001b[0;32m----> 4\u001b[0m image, x_t \u001b[38;5;241m=\u001b[39m run_and_display(prompts, controller, latent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, run_baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, generator\u001b[38;5;241m=\u001b[39mg_cpu)\n\u001b[1;32m      5\u001b[0m show_cross_attention(controller, res\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, from_where\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdown\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mrun_and_display\u001b[0;34m(prompts, controller, latent, run_baseline, generator)\u001b[0m\n\u001b[1;32m      4\u001b[0m     images, latent \u001b[38;5;241m=\u001b[39m run_and_display(prompts, EmptyControl(), latent\u001b[38;5;241m=\u001b[39mlatent, run_baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, generator\u001b[38;5;241m=\u001b[39mgenerator)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith prompt-to-prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m images, x_t \u001b[38;5;241m=\u001b[39m ptp_utils\u001b[38;5;241m.\u001b[39mtext2image_ldm_stable(ldm_stable, prompts, controller, latent\u001b[38;5;241m=\u001b[39mlatent, num_inference_steps\u001b[38;5;241m=\u001b[39mNUM_DIFFUSION_STEPS, guidance_scale\u001b[38;5;241m=\u001b[39mGUIDANCE_SCALE, generator\u001b[38;5;241m=\u001b[39mgenerator, low_resource\u001b[38;5;241m=\u001b[39mLOW_RESOURCE)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#ptp_utils.view_images(images)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images, x_t\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:166\u001b[0m, in \u001b[0;36mtext2image_ldm_stable\u001b[0;34m(model, prompt, controller, num_inference_steps, guidance_scale, generator, latent, low_resource)\u001b[0m\n\u001b[1;32m    164\u001b[0m model\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mset_timesteps(num_inference_steps)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(model\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mtimesteps):\n\u001b[0;32m--> 166\u001b[0m     latents \u001b[38;5;241m=\u001b[39m diffusion_step(model, controller, latents, context, t, guidance_scale, low_resource)\n\u001b[1;32m    168\u001b[0m image \u001b[38;5;241m=\u001b[39m latent2image(model\u001b[38;5;241m.\u001b[39mvae, latents)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, latent\n",
      "File \u001b[0;32m~/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:70\u001b[0m, in \u001b[0;36mdiffusion_step\u001b[0;34m(model, controller, latents, context, t, guidance_scale, low_resource)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     latents_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39munet(latents_input, t, encoder_hidden_states\u001b[38;5;241m=\u001b[39mcontext)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m     noise_pred_uncond, noise_prediction_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     72\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m noise_pred_uncond \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (noise_prediction_text \u001b[38;5;241m-\u001b[39m noise_pred_uncond)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py:1214\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1212\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1214\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(\n\u001b[1;32m   1215\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[1;32m   1216\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[1;32m   1217\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1218\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1219\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m   1220\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_residuals,\n\u001b[1;32m   1222\u001b[0m     )\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1224\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:1270\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1269\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m-> 1270\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn(\n\u001b[1;32m   1271\u001b[0m         hidden_states,\n\u001b[1;32m   1272\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1273\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m   1274\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1275\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1276\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1277\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/diffusers/models/transformers/transformer_2d.py:427\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    416\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    417\u001b[0m             block,\n\u001b[1;32m    418\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m             class_labels,\n\u001b[1;32m    425\u001b[0m         )\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    428\u001b[0m             hidden_states,\n\u001b[1;32m    429\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    430\u001b[0m             encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    431\u001b[0m             encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    432\u001b[0m             timestep\u001b[38;5;241m=\u001b[39mtimestep,\n\u001b[1;32m    433\u001b[0m             cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m    434\u001b[0m             class_labels\u001b[38;5;241m=\u001b[39mclass_labels,\n\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/diffusers/models/attention.py:514\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    512\u001b[0m gligen_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 514\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[1;32m    515\u001b[0m     norm_hidden_states,\n\u001b[1;32m    516\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    517\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[1;32m    519\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    522\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:220\u001b[0m, in \u001b[0;36mregister_attention_control.<locals>.ca_forward.<locals>.forward\u001b[0;34m(hidden_states, encoder_hidden_states, attention_mask, temb)\u001b[0m\n\u001b[1;32m    217\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attention_scores(query, key, attention_mask)\n\u001b[1;32m    218\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m controller(attention_probs, is_cross, place_in_unet)\n\u001b[0;32m--> 220\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_probs, value)\n\u001b[1;32m    221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_to_head_dim(hidden_states)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# linear proj\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_cpu = torch.Generator().manual_seed(8888)\n",
    "prompts = [\"A painting of a squirrel eating a burger\"]\n",
    "controller = AttentionStore()\n",
    "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
    "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A painting of a lion eating a burger\"]\n",
    "\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Caption: A couple stands on a beach, with waves in the background. The person on the left wears a light top and shorts, while the person on the right is shirtless, wearing shorts. The setting sun casts a muted light over the scene.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/architg/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:90: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (1, model.unet.in_channels, height // 8, width // 8),\n",
      "/Users/architg/Git/REALEDITPLUS/P2P_SD_1.4_Pipeline/ptp_utils.py:93: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  latents = latent.expand(batch_size,  model.unet.in_channels, height // 8, width // 8).to(model.device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb8abd37b1430886c2d6f172d51071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2af9e5588a44548089fc22db5ce9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(cap_and_edits)\n",
    "#print(\"OG Caption: \" + str(cap_and_edits))\n",
    "save_root = \"dataset\"\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "for _, captions in cap_and_edits.items():\n",
    "    og_cap = captions[0]\n",
    "    edited_caps = captions[1]\n",
    "\n",
    "    print(\"OG Caption: \" + og_cap)\n",
    "\n",
    "    g_cpu = torch.Generator().manual_seed(8888)\n",
    "    prompts = [og_cap]\n",
    "    controller = AttentionStore()\n",
    "    original_image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
    "    original_pil = Image.fromarray(original_image[0])\n",
    "    \n",
    "\n",
    "    for instr, ec in edited_caps:\n",
    "        #print(\"INSTRUCTION:\" + str(instr))\n",
    "        #print(\"EDITED Caption:\" + str(ec))\n",
    "        #os.makedirs(\"first_img_edits\", exist_ok=True)\n",
    "        #img = Image.fromarray(image[0])  # image[0] = (H, W, C), dtype=uint8\n",
    "        prompts = [og_cap, ec]\n",
    "\n",
    "        #controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
    "        #_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n",
    "\n",
    "\n",
    "        controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.5, self_replace_steps=.2)\n",
    "        edited_image, _ = run_and_display(prompts, controller, latent=x_t)\n",
    "        edited_pil = Image.fromarray(edited_image[0])\n",
    "\n",
    "        sample_dir = os.path.join(save_root, f\"sample_{sample_idx:04d}\")\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "        original_pil.save(os.path.join(sample_dir, \"original.png\"))\n",
    "        edited_pil.save(os.path.join(sample_dir, \"edited.png\"))\n",
    "\n",
    "        with open(os.path.join(sample_dir, \"original_caption.txt\"), 'w') as f:\n",
    "            f.write(og_cap)\n",
    "\n",
    "        with open(os.path.join(sample_dir, \"edited_caption.txt\"), 'w') as f:\n",
    "            f.write(ec)\n",
    "\n",
    "        with open(os.path.join(sample_dir, \"instruction.txt\"), 'w') as f:\n",
    "            f.write(instr)\n",
    "\n",
    "        sample_idx += 1\n",
    "\n",
    "        #comment out to run on all edit instructions\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    # comment out to run on all input captions\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
