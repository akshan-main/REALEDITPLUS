{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SByBaTZjhUVH"
      },
      "outputs": [],
      "source": [
        "!pip install safetensors xformers pillow torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate openai open_clip_torch py-real-esrgan huggingface_hub\n"
      ],
      "metadata": {
        "id": "oi5XBtCnhW9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/from huggingface_hub import hf_hub_url, cached_download/from huggingface_hub import hf_hub_download, hf_hub_url/' /usr/local/lib/python3.11/dist-packages/py_real_esrgan/model.py\n",
        "!sed -i 's/cached_download(hf_hub_url(repo_id, filename))/hf_hub_download(repo_id=repo_id, filename=filename)/' /usr/local/lib/python3.11/dist-packages/py_real_esrgan/model.py\n"
      ],
      "metadata": {
        "id": "zXILGCE0hXMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, requests, os\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import openai\n",
        "from py_real_esrgan.model import RealESRGAN\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "glAhWuPxhXYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = #put the api here"
      ],
      "metadata": {
        "id": "TOSW715AhXjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "7pMAv9R-hXvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def ensure_direct_image_url(url):\n",
        "    if \"imgur.com\" in url and not re.search(r'\\.(jpg|jpeg|png|gif|bmp|webp|tiff)$', url, re.IGNORECASE):\n",
        "        match = re.search(r'imgur\\.com/(?:gallery/|a/)?([^/?#]+)', url)\n",
        "        if match:\n",
        "            return f\"https://i.imgur.com/{match.group(1)}.jpg\"\n",
        "        match = re.search(r'imgur\\.com/([^/?#]+)', url)\n",
        "        if match:\n",
        "            return f\"https://i.imgur.com/{match.group(1)}.jpg\"\n",
        "    return url\n",
        "\n",
        "def smart_download_image(url, save_path):\n",
        "    if \"dropbox.com\" in url:\n",
        "        url = url.replace(\"?dl=0\", \"\")\n",
        "        if \"?raw=1\" not in url:\n",
        "            url += \"&raw=1\" if \"?\" not in url else \"&raw=1\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\",\n",
        "        \"Referer\": url,\n",
        "        \"Accept-Encoding\": \"identity\",\n",
        "        \"Connection\": \"keep-alive\"\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=30)\n",
        "        if resp.status_code == 200 and resp.headers.get('content-type', '').startswith(\"image\"):\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                f.write(resp.content)\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"Download error for {url}: {e}\")\n",
        "    return False\n",
        "\n",
        "def generate_blip_caption(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device)\n",
        "        output = blip_model.generate(**inputs, max_length=60, num_beams=11,\n",
        "                                     length_penalty=1.7, repetition_penalty=1.4, early_stopping=True, do_sample=False)\n",
        "        caption = blip_processor.decode(output[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        print(f\"BLIP failed for {image_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "df = pd.read_csv(\"RealEdit_train_split_urls.csv\")\n",
        "N = 10\n",
        "image_urls = []\n",
        "original_captions = []\n",
        "os.makedirs(\"originals\", exist_ok=True)\n",
        "BAD_CAPTION_KEYWORDS = [\"image you are requesting\", \"not available\", \"doesn’t exist\", \"doesn't exist\", \"no longer available\", \"broken image\", \"missing image\", \"404\"]\n",
        "\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=min(len(df), N)):\n",
        "    if i >= N:\n",
        "        break\n",
        "    filename = row[\"input_image_name\"]\n",
        "    orig_url = ensure_direct_image_url(str(row[\"input_url\"]))\n",
        "    save_path = f\"originals/{filename}\"\n",
        "    if smart_download_image(orig_url, save_path):\n",
        "        caption = generate_blip_caption(save_path)\n",
        "        image_urls.append((filename, orig_url))\n",
        "        original_captions.append(caption)\n",
        "        print(f\"{filename} BLIP caption: {caption}\")\n",
        "        caption_clean = caption.lower()\n",
        "        if any(bad_phrase in caption_clean for bad_phrase in BAD_CAPTION_KEYWORDS):\n",
        "          print(f\"Skipping {filename} due to invalid BLIP caption: '{caption}'\")\n",
        "          continue\n",
        "    else:\n",
        "        print(f\"Skipping {filename} due to download failure.\")"
      ],
      "metadata": {
        "id": "xgY5PreKhX5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "I_A-sw1ghYIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "clip_model = clip_model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "2pm444yEiCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=openai.api_key)\n",
        "\n",
        "generated_edits = {}\n",
        "\n",
        "for (filename, url), caption in zip(image_urls, original_captions):\n",
        "    print(f\"\\nGenerating edit instructions for image '{filename}' with caption: {caption}\")\n",
        "    prompt = (\n",
        "    \"You are simulating real user editing behavior for a dataset of image edits.\\n\"\n",
        "    \"Given a description of an image, imagine how actual users would ask to modify it. \"\n",
        "    \"These edits should be creative, realistic, and specific; things a person might type into an AI editor, like:\\n\"\n",
        "    \"- 'Add a dog sitting near the woman'\\n\"\n",
        "    \"- 'Make the sunset more vibrant'\\n\"\n",
        "    \"- 'Change the man’s outfit to a business suit'\\n\"\n",
        "    \"- 'Remove the second person from the left'\\n\"\n",
        "    \"- 'Make the child look older'\\n\"\n",
        "    \"Each edit should involve a meaningful visual change to the image, not just generic filters like 'increase contrast'.\\n\"\n",
        "    \"\\n\"\n",
        "    \"For each instruction, generate a matching edited image caption that describes the image *after* the edit.\\n\"\n",
        "    \"Avoid repetitions. The 10 edits must be diverse (e.g. subject, background, object-level, style).\\n\"\n",
        "    \"\\n\"\n",
        "    \"Output a JSON array of 10 items, where each item is an object with two fields:\\n\"\n",
        "    \"- 'instruction': the user's edit request\\n\"\n",
        "    \"- 'edited_caption': the caption for the image after applying that edit\\n\"\n",
        "    \"Do not include any explanation. Return only the JSON array.\\n\\n\"\n",
        "    f\"Image Description: \\\"{caption}\\\"\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.8\n",
        "        )\n",
        "        gpt_output = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"GPT API call failed for {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "    json_str = \"\"\n",
        "    start_idx = gpt_output.find('[')\n",
        "    end_idx = gpt_output.rfind(']')\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        json_str = gpt_output[start_idx:end_idx + 1]\n",
        "    else:\n",
        "        json_str = gpt_output.strip()\n",
        "\n",
        "    try:\n",
        "        instructions_list = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON parsing failed for {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if not isinstance(instructions_list, list) or len(instructions_list) != 10:\n",
        "        print(f\"Unexpected format or not 10 items returned for {filename}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    generated_edits[filename] = instructions_list\n",
        "    for idx, item in enumerate(instructions_list, start=1):\n",
        "        print(f\" {idx}. {item['instruction']} -> Edited caption: {item['edited_caption']}\")\n"
      ],
      "metadata": {
        "id": "z2ykioKhiCjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}